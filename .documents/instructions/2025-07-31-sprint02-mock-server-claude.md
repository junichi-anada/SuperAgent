# 指示書: Sprint 2 - AI機能モックサーバー実装

## 1. 目的
ユーザーがAI機能の開発を円滑に進められるよう、バックエンドにAIの応答を模倣するモックサーバー機能を実装する。

## 2. 指示方針
- **技術スタック:** バックエンドはFastAPI、フロントエンドはNext.jsを使用すること。
- **参考資料:** プロジェクトの全体像や細かい仕様で迷ったら、必ず `.documents/Project.md` を確認すること。
- **コーディング規約:** PythonはPEP8、JavaScript/TypeScriptはPrettierの標準ルールに従うこと。
- **アウトプット形式:** 作業が完了したら、変更・追加したファイルのパスと、それぞれの作業内容の簡単なサマリーをJSON形式で報告すること。
- **進捗報告:** 以下の作業手順を1つ実行するごとに、「ステップX完了」と簡潔に報告すること。

## 3. 作業手順

### ステップ1: FastAPIモックAPIの実装
- **対象ファイル:** `backend/main.py`
- **作業内容:**
    - ファイルの末尾に、`/api/v1/chat/mock` というPOSTリクエスト用のエンドポイントを追加してください。
    - このAPIは、リクエストボディの内容に関わらず、常に以下のJSONレスポンスを返却するように実装してください。
      ```json
      {
        "message": "これはモックサーバーからの返信です。",
        "timestamp": "現在時刻のISO8601形式"
      }
      ```

### ステップ2: Docker Composeの更新
- **対象ファイル:** `docker-compose.yml`
- **作業内容:**
    - `services:` ブロックの末尾に、`ollama` と `comfyui` のサービス定義を追加してください。
    - これらのサービスは通常起動しないように、`profiles: ["gpu"]` を設定してください。これにより、`docker compose up --profile gpu` を実行した時だけ起動するようになります。

## 4. 完了報告
全ての作業が完了したら、最終的な成果物として、指示方針で定めた通りのJSON報告を行ってください。
